{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b071bc",
   "metadata": {},
   "source": [
    "# Assignment 05 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed445a",
   "metadata": {},
   "source": [
    "#### 1. How can each of these parameters be fine-tuned? â€¢ Number of hidden layers \n",
    "1. Network architecture (network depth)\n",
    "2. Each layer's number of neurons (layer width)\n",
    "3. Form of activation\n",
    "4. Optimization and learning\n",
    "5. Learning rate and decay schedule\n",
    "6. Mini batch size\n",
    "7. Algorithms for optimization\n",
    "8. The number of epochs (and early stopping criteria)\n",
    "9. Overfitting that be avoided by using regularization techniques.\n",
    "10. L2 normalization\n",
    "11. Drop out layers\n",
    "12. Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b02c7",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Number of hidden layers: Experiment with different layer counts to find the optimal trade-off between model complexity and generalization.\n",
    "\n",
    "Network architecture (network depth): Adjust the depth of the neural network by adding or removing layers to optimize performance.\n",
    "\n",
    "Each layer's number of neurons (layer width): Tune the number of neurons in each layer to balance model capacity and overfitting.\n",
    "\n",
    "Form of activation: Choose appropriate activation functions (e.g., ReLU, sigmoid) for each layer to enhance the network's learning capability.\n",
    "\n",
    "Optimization and learning: Select suitable optimization algorithms (e.g., Adam, SGD) to train the network more efficiently.\n",
    "\n",
    "Learning rate and decay schedule: Fine-tune the learning rate and decay schedule to control the rate of parameter updates and convergence.\n",
    "\n",
    "Mini batch size: Adjust the mini batch size to balance training efficiency and memory usage.\n",
    "\n",
    "Algorithms for optimization: Explore different optimization techniques (e.g., momentum, RMSProp) to speed up convergence and avoid local minima.\n",
    "\n",
    "The number of epochs (and early stopping criteria): Find the right balance by training for an optimal number of epochs and utilizing early stopping to prevent overfitting.\n",
    "\n",
    "Overfitting that be avoided by using regularization techniques: Apply techniques like dropout and L2 regularization to prevent overfitting by penalizing complex models.\n",
    "\n",
    "L2 normalization: Incorporate L2 normalization (weight decay) to control the magnitude of weight values and reduce overfitting.\n",
    "\n",
    "Dropout layers: Add dropout layers to randomly deactivate neurons during training, reducing overfitting by introducing noise.\n",
    "\n",
    "Data augmentation: Apply data augmentation techniques (e.g., rotation, flipping) to artificially increase the diversity of training data and improve generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
