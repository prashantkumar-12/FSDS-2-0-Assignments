{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d16d942",
   "metadata": {},
   "source": [
    "# Assignment 07 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afd3e4",
   "metadata": {},
   "source": [
    "#### 1. What is the COVARIATE SHIFT Issue, and how does it affect you?\n",
    "**Answer**\n",
    "COVARIATE SHIFT Issue: Covariate shift refers to the change in the distribution of input data between different layers during training. It affects learning as the model adapts to the varying data distribution, making it harder to converge and learn accurate features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531e180",
   "metadata": {},
   "source": [
    "#### 2. What is the process of BATCH NORMALIZATION?\n",
    "**Answer**\n",
    "BATCH NORMALIZATION: Batch normalization is a technique that normalizes the activations of a layer by calculating mean and variance within each mini-batch during training. It stabilizes training by reducing internal covariate shift, aiding faster convergence and improved generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732edecd",
   "metadata": {},
   "source": [
    "#### 3.Using our own terms and diagrams, explain LENET ARCHITECTURE.\n",
    "**Answer**\n",
    "LENET ARCHITECTURE: LeNet is an early convolutional neural network (CNN) architecture designed for handwritten digit recognition. It consists of two convolutional layers followed by pooling layers, and then fully connected layers for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ffbc56",
   "metadata": {},
   "source": [
    "#### 4.Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.\n",
    "**Answer**\n",
    "ALEXNET ARCHITECTURE: AlexNet is a pioneering CNN architecture for image classification. It comprises five convolutional layers with max-pooling, followed by three fully connected layers. It introduced concepts like ReLU activation, dropout, and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0a0ee",
   "metadata": {},
   "source": [
    "#### 5.Describe the vanishing gradient problem.\n",
    "**Answer**\n",
    "Vanishing Gradient Problem: The vanishing gradient problem occurs when gradients become very small as they propagate backward through many layers during training. This hinders the training of deep networks as weight updates become negligible, leading to slow convergence or stagnation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f03d80",
   "metadata": {},
   "source": [
    "#### 6.What is NORMALIZATION OF LOCAL RESPONSE?\n",
    "**Answer**\n",
    "Normalization of Local Response: Local response normalization (LRN) normalizes a neuron's activity by dividing it by a sum of the squares of neighboring neuron outputs. It enhances the contrast of neurons and encourages competition between adjacent neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d63f1b",
   "metadata": {},
   "source": [
    "#### 7.In AlexNet, what WEIGHT REGULARIZATION was used?\n",
    "**Answer**Weight Regularization in AlexNet: AlexNet used L2 weight regularization (weight decay) to penalize large weights and prevent overfitting. It adds the squared magnitude of weights to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c252699",
   "metadata": {},
   "source": [
    "#### 8.Using our own terms and diagrams, explain VGGNET ARCHITECTURE.\n",
    "**Answer**VGGNET ARCHITECTURE: VGGNet is a deep CNN architecture with a focus on using small 3x3 convolutional filters to create a deep and uniform network. It has various configurations (e.g., VGG16, VGG19) with multiple convolutional and fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b33000",
   "metadata": {},
   "source": [
    "#### 9.Describe VGGNET CONFIGURATIONS.\n",
    "\n",
    "\n",
    "**Answer**VGGNet Configurations: VGGNet has different configurations with varying depths and numbers of layers, typically having multiple convolutional layers (up to 19) followed by fully connected layers for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18c6c3",
   "metadata": {},
   "source": [
    "#### 10.What regularization methods are used in VGGNET to prevent overfitting?\n",
    "**Answer**Regularization Methods in VGGNET: VGGNet uses dropout and L2 weight regularization to prevent overfitting. Dropout randomly deactivates neurons during training, while weight regularization adds a penalty for large weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1504513f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f519aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dea9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
