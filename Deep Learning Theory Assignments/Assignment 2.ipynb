{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 02 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a302866",
   "metadata": {},
   "source": [
    "**Answer**The structure of an artificial neuron, also known as a node or a perceptron, is inspired by the biological neuron but is a simplified computational model used in artificial neural networks. While not a perfect replica of a biological neuron, it shares some similarities and essential components.\n",
    "\n",
    "The main components of an artificial neuron are as follows:\n",
    "a. Inputs: An artificial neuron receives input signals from multiple sources. Each input is associated with a weight, representing the importance of that input to the neuron's output.\n",
    "\n",
    "b. Weights: Weights are numerical values that modify the strength of individual inputs. They determine how much influence each input has on the neuron's output. During training, the neural network adjusts these weights to learn from data and improve its performance.\n",
    "\n",
    "c. Summation Function: The inputs, each multiplied by their corresponding weights, are summed up in a linear combination. This summation operation is also known as the weighted sum or net input.\n",
    "\n",
    "d. Activation Function: After the summation, the result is passed through an activation function. The activation function introduces non-linearity to the neuron's output, allowing the artificial neuron to model complex relationships in the data.\n",
    "\n",
    "e. Output: The output of the neuron is the result of the activation function's computation. Depending on the problem type, the output may be binary (0 or 1), continuous (real numbers), or categorical (class labels).\n",
    "\n",
    "The similarities to a biological neuron include the idea of receiving inputs, integrating them, and producing an output based on a threshold or activation level. However, the artificial neuron is a simplified version, lacking the complexity and biological processes that occur in real neurons, such as action potentials and neurotransmitter release."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWhat are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c576b24",
   "metadata": {},
   "source": [
    "**Answer**Different types of activation functions used in artificial neural networks:\n",
    "a. Step Function: The step function is a basic activation function that outputs a constant value (usually 0 or 1) based on whether the input is greater than or equal to a threshold. It is discontinuous and provides a binary output, making it mainly useful for binary classification tasks. However, it is not commonly used in modern neural networks due to its non-differentiability, which makes gradient-based optimization difficult.\n",
    "\n",
    "b. Sigmoid Function (Logistic Function): The sigmoid function takes an input and maps it to a value between 0 and 1. It has a characteristic S-shaped curve and is widely used in the past for activation functions. However, it suffers from the vanishing gradient problem, which can slow down training, especially in deep networks.\n",
    "\n",
    "c. Hyperbolic Tangent Function (Tanh): Similar to the sigmoid function, the tanh function also maps the input to a value between -1 and 1. It is an improvement over the sigmoid as it has a steeper gradient, which helps mitigate the vanishing gradient problem to some extent.\n",
    "\n",
    "d. Rectified Linear Unit (ReLU): ReLU is a popular activation function used in deep learning models. It returns the input value if it is positive, and zero otherwise. ReLU helps address the vanishing gradient problem, is computationally efficient, and allows for faster convergence during training. However, it may suffer from the \"dying ReLU\" problem when some neurons get stuck at zero and do not update during training.\n",
    "\n",
    "e. Leaky Rectified Linear Unit (Leaky ReLU): To overcome the \"dying ReLU\" problem, the Leaky ReLU introduces a small slope for negative inputs, allowing a small, non-zero output for negative values. It maintains some level of gradient flow for negative inputs during backpropagation.\n",
    "\n",
    "f. Parametric Rectified Linear Unit (PReLU): Similar to Leaky ReLU, PReLU introduces a small slope for negative inputs. However, unlike Leaky ReLU, the slope is learned during training, allowing it to adapt to the data.\n",
    "\n",
    "g. Exponential Linear Unit (ELU): ELU is another variation of ReLU that returns the input for positive values and uses an exponential function for negative values. It has smoother derivatives than ReLU, which can lead to more stable training.\n",
    "\n",
    "Each activation function has its strengths and weaknesses, and the choice of activation function depends on the specific problem and the characteristics of the data. ReLU and its variants are widely used in modern deep learning architectures due to their efficiency and ability to mitigate gradient-related issues.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### Explain the below statements ?\n",
    "1.\tExplain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
    "2.\tUse a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f6b5b9",
   "metadata": {},
   "source": [
    "**Answer**a. Rosenblatt's perceptron model, proposed by Frank Rosenblatt in 1957, is one of the earliest neural network models. It is a type of single-layer feedforward neural network designed for binary classification tasks. The perceptron model is based on the concept of a linear threshold unit, which receives multiple inputs and produces a binary output (0 or 1) based on whether the weighted sum of inputs exceeds a certain threshold.\n",
    "\n",
    "The perceptron model can be mathematically represented as follows:\n",
    "\n",
    "Input: Let's say we have n input features, denoted as x1, x2, ..., xn. Each data point is represented as a vector of these input features: (x1, x2, ..., xn).\n",
    "\n",
    "Weights: The perceptron has corresponding weights for each input feature, denoted as w1, w2, ..., wn. The weights determine the significance of each input feature.\n",
    "\n",
    "Bias: The perceptron also has a bias term, denoted as w0. The bias allows the perceptron to shift the decision boundary.\n",
    "\n",
    "Activation Function: The activation function in Rosenblatt's perceptron is the step function, also known as the Heaviside step function. The output of the perceptron is determined by whether the weighted sum of inputs plus the bias exceeds a certain threshold. If the sum is greater than or equal to the threshold, the output is 1 (positive class); otherwise, it is 0 (negative class).\n",
    "\n",
    "Training the Perceptron:\n",
    "\n",
    "Initialize the weights and bias to small random values.\n",
    "Iterate over the training data points until convergence or a specified number of epochs:\n",
    "a. For each data point (x1, x2, ..., xn):\n",
    "b. Compute the weighted sum of inputs: z = w0 + w1x1 + w2x2 + ... + wn*xn.\n",
    "c. Apply the activation function: If z ≥ 0, the output is 1; otherwise, the output is 0.\n",
    "d. Update the weights and bias using the perceptron learning rule:\n",
    "If the predicted output is correct (matches the true label), no update is needed.\n",
    "If the predicted output is 1, but the true label is 0, decrease the weights: wi = wi - α * xi.\n",
    "If the predicted output is 0, but the true label is 1, increase the weights: wi = wi + α * xi.\n",
    "Update the bias: w0 = w0 ± α (bias has no corresponding input xi).\n",
    "e. Repeat the process until the model converges or the maximum number of epochs is reached.\n",
    "b. Using the given weights w0 = -1, w1 = 2, and w2 = 1, we can classify the data points as follows:\n",
    "\n",
    "Data Points: (3, 4), (5, 2), (1, -3), (-8, -3), (-3, 0)\n",
    "\n",
    "For each data point (x1, x2):\n",
    "\n",
    "Compute the weighted sum of inputs: z = w0 + w1x1 + w2x2.\n",
    "Apply the step function (activation function): If z ≥ 0, the output is 1; otherwise, the output is 0.\n",
    "a. For (3, 4):\n",
    "z = -1 + 23 + 14 = 1.\n",
    "Output = 1.\n",
    "\n",
    "b. For (5, 2):\n",
    "z = -1 + 25 + 12 = 11.\n",
    "Output = 1.\n",
    "\n",
    "c. For (1, -3):\n",
    "z = -1 + 21 + 1(-3) = 0.\n",
    "Output = 1.\n",
    "\n",
    "d. For (-8, -3):\n",
    "z = -1 + 2*(-8) + 1*(-3) = -21.\n",
    "Output = 0.\n",
    "\n",
    "e. For (-3, 0):\n",
    "z = -1 + 2*(-3) + 1*0 = -7.\n",
    "Output = 0.\n",
    "\n",
    "So, the data points (3, 4), (5, 2), and (1, -3) are classified as positive (1), while the data points (-8, -3) and (-3, 0) are classified as negative (0) using the given perceptron with weights w0 = -1, w1 = 2, and w2 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78cf38",
   "metadata": {},
   "source": [
    "**Answer**Multi-Layer Perceptron (MLP) is a type of artificial neural network with multiple layers, including an input layer, one or more hidden layers, and an output layer. Each layer contains multiple neurons (nodes), and neurons in different layers are interconnected with weighted connections. The basic structure of an MLP can be summarized as follows:\n",
    "Input Layer: The input layer receives the input data and passes it forward to the hidden layers.\n",
    "\n",
    "Hidden Layers: Hidden layers are intermediary layers between the input and output layers. They perform nonlinear transformations on the input data, allowing the network to learn complex patterns and relationships.\n",
    "\n",
    "Neurons: Each neuron in the hidden and output layers computes a weighted sum of its inputs and applies an activation function to produce the output.\n",
    "\n",
    "Weights: The connections between neurons in different layers have associated weights. These weights are learned during the training process and determine the strength of the connections.\n",
    "\n",
    "Bias: Each neuron in the hidden and output layers also has an associated bias, which allows the network to shift its decision boundary.\n",
    "\n",
    "\n",
    "Solving the XOR problem using an MLP:\n",
    "The XOR problem is not linearly separable and cannot be solved by a single-layer perceptron. However, an MLP with at least one hidden layer can solve it effectively. The hidden layer introduces non-linearity, allowing the network to learn the XOR function.\n",
    "\n",
    "Suppose we have a simple XOR problem with two binary inputs (A and B) and one binary output (Y). The XOR function outputs 1 when the inputs are different and 0 when they are the same (A XOR B).\n",
    "\n",
    "Input Layer: Two input neurons (A and B) representing the binary inputs.\n",
    "Hidden Layer: Two neurons with non-linear activation functions (e.g., ReLU or sigmoid).\n",
    "Output Layer: One neuron with a sigmoid activation function for the binary output.\n",
    "During training, the MLP adjusts the weights and biases using backpropagation to minimize the error between the predicted output and the true labels. With appropriate weight adjustments, the MLP can learn to accurately classify XOR inputs and produce the correct output for each input combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee862af",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Artificial Neural Network (ANN) is a computational model inspired by the structure and function of biological neural networks in the brain. It is composed of interconnected nodes (neurons) organized into layers. ANN can be used for various tasks, such as pattern recognition, regression, classification, and reinforcement learning.\n",
    "Salient highlights in different architectural options for ANN:\n",
    "\n",
    "Multi-Layer Perceptron (MLP): Consists of an input layer, one or more hidden layers, and an output layer. It is suitable for handling complex and nonlinear problems.\n",
    "\n",
    "Convolutional Neural Network (CNN): Specialized for image and video analysis. Utilizes convolutional layers for feature extraction and pooling layers for down-sampling.\n",
    "\n",
    "Recurrent Neural Network (RNN): Designed to handle sequential data by introducing feedback loops. Suitable for tasks like language modeling and time series prediction.\n",
    "\n",
    "Long Short-Term Memory (LSTM): A type of RNN with memory cells, allowing it to capture long-term dependencies in sequential data.\n",
    "\n",
    "Generative Adversarial Network (GAN): Comprises a generator and a discriminator, used for generating realistic data, such as images, from random noise.\n",
    "\n",
    "Autoencoder: Utilizes unsupervised learning to learn efficient representations of data by encoding it into a lower-dimensional space and decoding it back to the original data format.\n",
    "\n",
    "Transformer: Introduced attention mechanisms for handling sequences of data, widely used in natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tExplain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b02cab",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "The learning process of an ANN involves adjusting the synaptic weights (connections between neurons) based on training data. The challenge in assigning synaptic weights lies in determining suitable initial values for the weights, especially in large neural networks.\n",
    "Challenge: The large number of weights in deep neural networks makes it challenging to initialize them in a way that facilitates efficient and effective learning. If the weights are set too small, the signals may become too weak during forward propagation, leading to slow learning or vanishing gradients. On the other hand, if the weights are set too large, the signals can become too strong, leading to unstable and oscillating learning.\n",
    "\n",
    "Addressing the challenge: There are several strategies to address the weight initialization challenge:\n",
    "\n",
    "Random Initialization: One common approach is to initialize weights randomly with small values centered around zero. This helps prevent symmetry issues in the network and allows different neurons to learn different features.\n",
    "\n",
    "Xavier/Glorot Initialization: This method sets the initial weights based on the number of input and output connections to a neuron, providing a better balance for different layers' activations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tExplain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaae8b4",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Backpropagation Algorithm:\n",
    "Backpropagation is an optimization algorithm used to train multi-layer neural networks by adjusting the weights to minimize the error between predicted outputs and target outputs during training. The algorithm is based on the chain rule of calculus, allowing the gradients of the loss function with respect to the weights to be propagated backward through the network.\n",
    "\n",
    "Steps of the backpropagation algorithm:\n",
    "\n",
    "Forward Propagation: Input data is fed through the network, and activations are computed layer by layer from the input layer to the output layer.\n",
    "\n",
    "Compute Loss: The difference between the predicted output and the target output is calculated using a loss function (e.g., Mean Squared Error or Cross-Entropy).\n",
    "\n",
    "Backward Propagation: The gradients of the loss function with respect to the weights are computed by applying the chain rule, propagating the error backward from the output layer to the input layer.\n",
    "\n",
    "Update Weights: The computed gradients are used to update the weights in the network using an optimization algorithm (e.g., Gradient Descent) to minimize the loss.\n",
    "Limitations of the backpropagation algorithm:\n",
    "\n",
    "Vanishing Gradient: The gradients can become extremely small as they propagate through deep networks with many layers, leading to slow or ineffective learning.\n",
    "\n",
    "Overfitting: Backpropagation can lead to overfitting if the model becomes too complex and memorizes the training data instead of generalizing to unseen data.\n",
    "\n",
    "Local Minima: The optimization process can get stuck in local minima, resulting in suboptimal solutions.\n",
    "\n",
    "Computational Cost: The computation of gradients for large networks can be computationally expensive and time-consuming.\n",
    "\n",
    "Adjusting Interconnection Weights in a Multi-Layer Neural Network:\n",
    "\n",
    "Initialization: The weights are initialized randomly, typically with small values centered around zero.\n",
    "\n",
    "Forward Propagation: Input data is fed through the network, and activations are computed layer by layer using the current weights.\n",
    "\n",
    "Loss Calculation: The difference between the predicted output and the target output is calculated using a loss function.\n",
    "\n",
    "Backward Propagation: The gradients of the loss function with respect to the weights are computed using the chain rule, propagating the error backward through the network.\n",
    "\n",
    "Weight Update: The gradients are used to update the weights in the network, typically using an optimization algorithm like Gradient Descent or its variants. The weights are adjusted in the direction that minimizes the loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tDescribe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f15fe5",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Process of Adjusting Interconnection Weights in a Multi-Layer Neural Network:\n",
    "The process of adjusting interconnection weights in a multi-layer neural network is a critical step in the training process, aiming to minimize the difference between the predicted output and the target output (loss function). The weights are adjusted using an optimization algorithm like Gradient Descent or its variants. Here's a detailed description of the process:\n",
    "\n",
    "Initialization: The weights in the network are initialized randomly, typically with small values centered around zero. It is essential to initialize weights carefully to avoid issues like vanishing or exploding gradients.\n",
    "\n",
    "Forward Propagation: Input data is fed into the network, and activations are computed layer by layer using the current weights. Starting from the input layer, the output of each neuron is calculated using the weighted sum of its inputs and applying the activation function. The output is passed to the next layer as input.\n",
    "\n",
    "Loss Calculation: The difference between the predicted output and the target output (ground truth) is calculated using a loss function, such as Mean Squared Error (MSE) for regression tasks or Cross-Entropy Loss for classification tasks.\n",
    "\n",
    "Backward Propagation (Backpropagation): The gradients of the loss function with respect to the weights are computed using the chain rule of calculus. Backpropagation involves propagating the error backward from the output layer to the input layer. The gradients indicate how much the loss function would change with respect to a small change in each weight.\n",
    "\n",
    "Weight Update: The computed gradients are used to update the weights in the network, typically using an optimization algorithm like Gradient Descent. The weights are adjusted in the direction that minimizes the loss function. The update rule can be formulated as:\n",
    "\n",
    "new_weight = old_weight - learning_rate * gradient\n",
    "\n",
    "where learning_rate is a hyperparameter that controls the step size during weight updates.\n",
    "\n",
    "\n",
    "\n",
    "The training process continues until the loss function reaches a sufficiently low value or until a predefined number of epochs is reached. After training, the network's weights represent the learned relationships between inputs and outputs, enabling the network to make accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tWhat are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a318f42",
   "metadata": {},
   "source": [
    "**Answer**Steps in the Backpropagation Algorithm and the Need for Multi-Layer Neural Networks:\n",
    "The Backpropagation algorithm comprises the following steps:\n",
    "\n",
    "Forward Propagation: Input data is fed through the network, and activations are computed layer by layer from the input layer to the output layer.\n",
    "\n",
    "Compute Loss: The difference between the predicted output and the target output is calculated using a loss function (e.g., Mean Squared Error or Cross-Entropy).\n",
    "\n",
    "Backward Propagation: The gradients of the loss function with respect to the weights are computed by applying the chain rule of calculus. The error is propagated backward from the output layer to the input layer.\n",
    "\n",
    "Update Weights: The computed gradients are used to update the weights in the network using an optimization algorithm (e.g., Gradient Descent) to minimize the loss.\n",
    "\n",
    "The Backpropagation algorithm is essential for training multi-layer neural networks because it allows the network to learn from data and adjust its weights to approximate complex mappings between inputs and outputs. Without multiple layers (i.e., a single-layer neural network), the network would be limited to representing only linear mappings, severely restricting its ability to learn and model complex patterns and relationships in the data.\n",
    "\n",
    "By introducing hidden layers with non-linear activation functions, multi-layer neural networks can learn and represent highly non-linear functions, making them suitable for a wide range of tasks, including image recognition, natural language processing, and pattern recognition. The Backpropagation algorithm enables these multi-layer networks to efficiently adjust their weights to fit the training data and generalize well to unseen data, making them powerful tools for various machine learning tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tWrite short notes on:\n",
    "1.\tArtificial neuron\n",
    "2.\tMulti-layer perceptron\n",
    "3.\tDeep learning\n",
    "4.\tLearning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de440920",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Artificial Neuron:\n",
    "An artificial neuron, also known as a node or a perceptron, is the basic building block of artificial neural networks. It receives multiple input signals, each associated with a weight, and computes the weighted sum of inputs. The result is then passed through an activation function, which introduces non-linearity to the output. The activation function determines whether the neuron fires or remains inactive based on the summed input. Artificial neurons are interconnected to form layers, and multiple layers constitute an artificial neural network.\n",
    "\n",
    "Multi-layer Perceptron (MLP):\n",
    "A multi-layer perceptron is a type of artificial neural network with multiple layers, including an input layer, one or more hidden layers, and an output layer. Each neuron in the hidden and output layers computes a weighted sum of its inputs and applies an activation function to produce the output. The hidden layers introduce non-linearity to the network, enabling it to learn complex patterns and solve tasks that are not linearly separable.\n",
    "\n",
    "Deep Learning:\n",
    "Deep learning is a subfield of machine learning that focuses on training deep neural networks with multiple hidden layers (deep architectures). It aims to automatically learn hierarchical representations of data, allowing the network to extract increasingly abstract features from the input data. Deep learning has demonstrated outstanding performance in various domains, such as computer vision, natural language processing, and speech recognition.\n",
    "\n",
    "Learning Rate:\n",
    "The learning rate is a hyperparameter in machine learning algorithms that controls the step size of weight updates during training. It determines how much the weights should be adjusted in the direction of the computed gradients. A high learning rate may lead to rapid learning but can cause instability and overshooting. On the other hand, a low learning rate may lead to slow convergence or getting stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f89d0c9",
   "metadata": {},
   "source": [
    "#### 11.\tWrite the difference between:-\n",
    "1.\tActivation function vs threshold function\n",
    "2.\tStep function vs sigmoid function\n",
    "3.\tSingle layer vs multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d8a5e",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Activation function vs. Threshold function:\n",
    "Activation Function:\n",
    "\n",
    "Activation functions introduce non-linearity to the output of artificial neurons in neural networks.\n",
    "They help the network learn complex patterns and relationships in the data.\n",
    "Common activation functions include ReLU, sigmoid, tanh, and softmax.\n",
    "Threshold Function:\n",
    "\n",
    "The threshold function is a simple activation function used in early artificial neural networks like the perceptron.\n",
    "It returns a constant value (usually 0 or 1) based on whether the weighted sum of inputs exceeds a threshold.\n",
    "The threshold function is discontinuous and not differentiable, limiting its use in modern neural networks.\n",
    "Step function vs. Sigmoid function:\n",
    "Step Function:\n",
    "The step function, also known as the Heaviside step function, is a basic mathematical function.\n",
    "It outputs 0 for any input less than zero and 1 for any input greater than or equal to zero.\n",
    "The step function is binary and not continuous, making it unsuitable for backpropagation-based training algorithms.\n",
    "Sigmoid Function:\n",
    "\n",
    "The sigmoid function is a smooth, S-shaped activation function.\n",
    "It maps the input to a value between 0 and 1, providing a continuous output.\n",
    "The sigmoid function was widely used in early neural networks but is less common in modern architectures due to the vanishing gradient problem.\n",
    "Single-layer vs. Multi-layer Perceptron:\n",
    "Single-layer Perceptron:\n",
    "Single-layer perceptron has only one layer, typically an input layer connected directly to an output layer.\n",
    "It can only solve linearly separable problems and lacks the ability to learn complex patterns.\n",
    "The activation function used is typically the step function.\n",
    "Multi-layer Perceptron (MLP):\n",
    "\n",
    "MLP consists of an input layer, one or more hidden layers, and an output layer.\n",
    "It can learn and approximate complex non-linear functions, making it suitable for a wide range of tasks.\n",
    "Non-linear activation functions (e.g., ReLU, sigmoid) are used in the hidden and output layers.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
