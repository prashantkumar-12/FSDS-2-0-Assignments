{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 16 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1. Explain the Activation Functions in your own language\n",
    "- a) sigmoid\n",
    "- b) tanh\n",
    "- c) ReLU\n",
    "- d) ELU\n",
    "- e) LeakyReLU\n",
    "- f) swish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a519816",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "a) Sigmoid:\n",
    "The sigmoid activation function squashes input values between 0 and 1. It's often used in binary classification tasks to convert the input into a probability-like value. However, it suffers from vanishing gradient problems for large inputs and is not used as much in deep networks today.\n",
    "\n",
    "b) Tanh (Hyperbolic Tangent):\n",
    "Tanh is similar to sigmoid but squashes input values between -1 and 1. It's often used in hidden layers of neural networks and can help mitigate the vanishing gradient problem of sigmoid. Tanh provides zero-centered outputs, which can help with network convergence.\n",
    "\n",
    "c) ReLU (Rectified Linear Unit):\n",
    "ReLU replaces negative input values with zero and leaves positive values unchanged. It's the most commonly used activation function due to its simplicity and effectiveness in preventing vanishing gradient. ReLU accelerates convergence and is computationally efficient.\n",
    "\n",
    "d) ELU (Exponential Linear Unit):\n",
    "ELU is a variation of ReLU that has a smooth curve for negative inputs, preventing the dying ReLU problem. It maintains the benefits of ReLU while also handling negative values gracefully. ELU can lead to faster convergence and better generalization.\n",
    "\n",
    "e) LeakyReLU (Leaky Rectified Linear Unit):\n",
    "LeakyReLU is another variation of ReLU where negative inputs are not fully squashed to zero, but instead leak a small value (like 0.01 times the input). This addresses the dying ReLU issue and can improve learning in deeper networks. Different variants, such as Parametric ReLU (PReLU), allow the leaky factor to be learned during training.\n",
    "Swish Activation:\n",
    "Swish is an activation function that smoothly combines the benefits of ReLU and sigmoid. It computes x * sigmoid(x) and can perform well in deep networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2. What happens when you increase or decrease the optimizer learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864e8f7",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Learning Rate Effects:\n",
    "\n",
    "Increase: Faster convergence initially, but can lead to oscillations or overshooting.\n",
    "Decrease: Slower convergence, but more stable and fine-tuned progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3. What happens when you increase the number of internal hidden neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b66470c",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Hidden Neurons Impact:\n",
    "Increasing hidden neurons can increase model complexity and capacity, potentially improving performance on complex tasks. However, too many neurons can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4. What happens when you increase the size of batch computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c4544",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Batch Size Influence:\n",
    "\n",
    "Increase: Faster training due to parallelization, but may converge to suboptimal solutions or regularize less.\n",
    "Decrease: Slower training, but more accurate updates and better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5. Why we adopt regularization to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51627f3a",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Regularization for Overfitting:\n",
    "Regularization techniques (e.g., L1/L2 regularization, dropout) constrain the model's parameters to prevent it from fitting the training data noise excessively, reducing the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6. What are loss and cost functions in deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c60d5",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Loss and Cost Functions:\n",
    "\n",
    "Loss function measures the model's performance on a single training instance.\n",
    "Cost function (or objective function) aggregates losses over the entire dataset, providing a measure of how well the model performs on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7. What do ou mean by underfitting in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31449a18",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Underfitting in Neural Networks:\n",
    "Underfitting occurs when the neural network fails to capture the underlying patterns in the data. It indicates the model is too simple or lacks the capacity to learn the task, resulting in poor training and validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e671f",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Use of Dropout:\n",
    "Dropout is used to prevent overfitting by randomly \"dropping out\" a fraction of neurons during training. This reduces interdependencies between neurons and encourages the network to learn more robust features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
