{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 13 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWhy is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8691b",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Logistic Regression vs. Perceptron:\n",
    "Logistic Regression is preferred over a classical Perceptron due to its probabilistic interpretation and ability to provide class probabilities. To make a Perceptron equivalent to Logistic Regression, replace the step function with the logistic (sigmoid) activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tWhy was the logistic activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab300d",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Logistic Activation Function in MLPs:\n",
    "The logistic activation function was crucial in training the first MLPs as it allowed gradients to flow through the network, enabling efficient backpropagation-based training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935a9fd",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Three Popular Activation Functions:\n",
    "\n",
    "ReLU (Rectified Linear Unit)\n",
    "Sigmoid\n",
    "Tanh (Hyperbolic Tangent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tSuppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "- What is the shape of the input matrix X?\n",
    "- What about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
    "- What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "- What is the shape of the network’s output matrix Y?\n",
    "- Write the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58444b54",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "MLP Layer Shapes:\n",
    "\n",
    "Shape of X: (batch_size, 10)\n",
    "Shape of Wh: (10, 50)\n",
    "Shape of bh: (50,)\n",
    "Shape of Wo: (50, 3)\n",
    "Shape of bo: (3,)\n",
    "Shape of Y: (batch_size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tHow many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82712580",
   "metadata": {},
   "source": [
    "**Answers**m\n",
    "Output Layer Neurons for Different Tasks:\n",
    "\n",
    "For binary classification (spam/ham): 1 neuron with sigmoid activation.\n",
    "For MNIST (10 classes): 10 neurons with softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de2ced",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Backpropagation and Reverse-Mode Autodiff:\n",
    "Backpropagation is a training algorithm for neural networks that computes gradients for weight updates using chain rule. Reverse-mode autodiff is a generalized method for efficiently computing gradients of a computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tCan you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d8993",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Hyperparameters in MLP:\n",
    "\n",
    "Learning rate\n",
    "Number of hidden layers and neurons\n",
    "Activation functions\n",
    "Batch size\n",
    "Regularization (dropout, L2)\n",
    "Initialization methods\n",
    "Optimization algorithm (SGD, Adam)\n",
    "Learning rate schedule\n",
    "To address overfitting, you could:\n",
    "\n",
    "Decrease the number of hidden neurons/layers\n",
    "Apply regularization techniques\n",
    "Adjust the learning rate\n",
    "Increase dropout rate\n",
    "Modify the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tTrain a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29922fe5",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Training Deep MLP on MNIST:\n",
    "Training and code execution are not possible in this text format, but here's a general outline:\n",
    "\n",
    "Load MNIST dataset\n",
    "Build a deep MLP architecture\n",
    "Define loss function and optimization algorithm\n",
    "Train the model with training loop\n",
    "Use checkpoints, summaries, and TensorBoard for monitoring\n",
    "Aim for over 98% precision by tuning hyperparameters and model architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
