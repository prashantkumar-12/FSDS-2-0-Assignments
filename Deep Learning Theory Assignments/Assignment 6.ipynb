{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 6 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bed419",
   "metadata": {},
   "source": [
    "**Answwer**The advantages of a Convolutional Neural Network (CNN) over a fully connected Deep Neural Network (DNN) for image classification are:\n",
    "\n",
    "Parameter Efficiency: CNNs use convolutional layers to share weights across local regions of an image, reducing the number of learnable parameters compared to fully connected DNNs. This makes CNNs more efficient in handling large image inputs.\n",
    "\n",
    "Spatial Hierarchies: CNNs capture spatial hierarchies in images through convolutional layers, learning local patterns and gradually aggregating them to represent higher-level features. This enables CNNs to better capture spatial relationships and local patterns in images.\n",
    "\n",
    "Translation Invariance: CNNs are translationally invariant, meaning they can recognize patterns regardless of their position in the image. This property is beneficial for tasks like image classification, where the position of objects may vary within an image.\n",
    "\n",
    "Feature Reuse: CNNs reuse learned features across different spatial locations in an image, allowing them to detect similar patterns in different regions efficiently. This leads to better generalization and robustness to variations in object positions.\n",
    "\n",
    "Reduced Overfitting: The weight sharing and pooling operations in CNNs contribute to regularization, helping to reduce overfitting and improve generalization on image classification tasks.\n",
    "\n",
    "Local Connectivity: CNNs exploit local connectivity by focusing on a small receptive field, allowing them to efficiently process spatially localized features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1d65d",
   "metadata": {},
   "source": [
    "**Answer** To calculate the total number of parameters in the CNN, we need to consider the number of weights and biases in each layer.\n",
    "\n",
    "Given:\n",
    "\n",
    "Three convolutional layers with 3 × 3 kernels each.\n",
    "A stride of 2.\n",
    "\"Same\" padding, which means the output feature maps will have the same spatial dimensions as the input.\n",
    "The number of parameters in each convolutional layer can be calculated as follows:\n",
    "\n",
    "First Convolutional Layer:\n",
    "\n",
    "Input channels (RGB): 3\n",
    "Output channels: 100\n",
    "Number of weights: 3 × 3 × 3 × 100 = 2700\n",
    "Number of biases: 100\n",
    "Total parameters in the first layer: 2700 + 100 = 2800\n",
    "Second Convolutional Layer:\n",
    "\n",
    "Input channels: 100\n",
    "Output channels: 200\n",
    "Number of weights: 3 × 3 × 100 × 200 = 180,000\n",
    "Number of biases: 200\n",
    "Total parameters in the second layer: 180,000 + 200 = 180,200\n",
    "Third Convolutional Layer:\n",
    "\n",
    "Input channels: 200\n",
    "Output channels: 400\n",
    "Number of weights: 3 × 3 × 200 × 400 = 720,000\n",
    "Number of biases: 400\n",
    "Total parameters in the third layer: 720,000 + 400 = 720,400\n",
    "Total number of parameters in the CNN:\n",
    "2800 + 180,200 + 720,400 = 903,400 parameters.\n",
    "\n",
    "To calculate the RAM required for a single prediction instance:\n",
    "Assuming 32-bit floats for each parameter (4 bytes per parameter):\n",
    "903,400 parameters × 4 bytes/parameter ≈ 3.4 MB\n",
    "\n",
    "When training on a mini-batch of 50 images:\n",
    "Assuming each image is loaded into memory separately for training (50 instances):\n",
    "3.4 MB × 50 ≈ 170 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cbf75",
   "metadata": {},
   "source": [
    "**Answer**If the GPU runs out of memory while training a CNN, here are five things you could try to solve the problem:\n",
    "\n",
    "Reduce Batch Size: Use a smaller batch size during training to reduce the memory required for storing intermediate activations and gradients.\n",
    "Reduce Model Size: Reduce the number of layers, feature maps, or neurons in the CNN to decrease the number of parameters and memory usage.\n",
    "Use Mixed Precision: Utilize mixed precision training, where weights are stored in lower precision (e.g., float16) to reduce memory consumption while maintaining accuracy.\n",
    "Gradient Accumulation: Accumulate gradients over multiple mini-batches before applying weight updates. This allows using a smaller batch size per iteration while still updating the model effectively.\n",
    "Memory Optimization Techniques: Utilize memory-efficient implementations or optimize memory usage in your code and model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad7d48",
   "metadata": {},
   "source": [
    "**Answer**A max pooling layer is preferred over a convolutional layer with the same stride for downsampling because max pooling reduces spatial dimensions without adding learnable parameters. It introduces no additional weights or biases, making it computationally efficient. Convolutional layers, on the other hand, introduce more parameters and may increase the computational load when downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhen would you want to add a local response normalization layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317818d",
   "metadata": {},
   "source": [
    "**Answer**Local Response Normalization (LRN) layers were commonly used in older CNN architectures like AlexNet to promote competition between neighboring feature maps. However, the use of LRN has decreased in modern architectures because batch normalization and other normalization techniques have proven more effective in stabilizing training and improving generalization. LRN may still be useful in specific cases where you want to encourage competition between nearby feature maps and have a specific architectural reason to use it, but it is no longer a standard layer in modern CNN architectures.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fe0c58",
   "metadata": {},
   "source": [
    "**Answer**Main innovations in AlexNet compared to LeNet-5:\n",
    "\n",
    "Larger Architecture: AlexNet has a deeper architecture with more layers, making it more capable of capturing complex patterns in images.\n",
    "ReLU Activation: AlexNet introduced the rectified linear unit (ReLU) activation function, which helps mitigate the vanishing gradient problem and accelerates convergence.\n",
    "Dropout Regularization: AlexNet used dropout regularization during training to reduce overfitting and improve generalization.\n",
    "Data Augmentation: AlexNet employed extensive data augmentation techniques, including random cropping, flipping, and color jittering, to increase the effective size of the training dataset and improve robustness.\n",
    "GPU Acceleration: AlexNet was one of the first CNN architectures to make use of GPUs for efficient training, enabling faster computations and larger model capacity.\n",
    "Main innovations in GoogLeNet (Inception), ResNet, SENet, and Xception:\n",
    "\n",
    "GoogLeNet: Introduced the Inception module, which uses multiple filter sizes (1x1, 3x3, 5x5) to capture features at different scales efficiently. It enabled training of deeper networks and was one of the first to explore the idea of \"network in network\" architectures.\n",
    "ResNet: Introduced residual connections (skip connections) to address the vanishing gradient problem in very deep networks. ResNet allowed for training of extremely deep networks (over a hundred layers) by facilitating the flow of gradients and easing the training process.\n",
    "SENet (Squeeze-and-Excitation Network): Introduced the concept of channel-wise attention, allowing the network to adaptively recalibrate the importance of each feature map channel. This attention mechanism improved feature selection and emphasized more relevant information.\n",
    "Xception: Introduced depthwise separable convolutions, which factorize a standard convolution into separate depthwise and pointwise convolutions. This reduced the number of parameters and computational cost while maintaining or even improving performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e70ad",
   "metadata": {},
   "source": [
    "**Answer**A fully convolutional network (FCN) is a neural network architecture designed for semantic segmentation tasks, where the goal is to assign a class label to each pixel in an input image. FCNs replace traditional fully connected layers with convolutional layers, allowing the network to take input of arbitrary sizes and produce output maps with spatial dimensions corresponding to the input.To convert a dense layer into a convolutional layer, you can use a 1x1 convolution with the same number of output channels as the number of neurons in the dense layer. This convolutional layer effectively acts as a global average pooling layer, aggregating information across the spatial dimensions and producing a 1x1 feature map for each channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tWhat is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a55755",
   "metadata": {},
   "source": [
    "**Answer**The main technical difficulty of semantic segmentation is preserving spatial information while handling variable-sized input images. Unlike image classification, where the input size is fixed, semantic segmentation requires the network to produce output maps with the same spatial dimensions as the input. This challenge arises due to the need to maintain spatial resolution throughout the network and to handle objects of various sizes and aspect ratios in the input images. FCNs and other modern segmentation architectures address this difficulty by using convolutional layers and upsampling techniques to produce dense predictions at the original image resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6d31b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.23.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (49.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pk664\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259ddd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 49s 19ms/step - loss: 0.2470 - accuracy: 0.9269 - val_loss: 0.0678 - val_accuracy: 0.9786\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0578 - accuracy: 0.9820 - val_loss: 0.0449 - val_accuracy: 0.9874\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0419 - accuracy: 0.9869 - val_loss: 0.0370 - val_accuracy: 0.9888\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0312 - accuracy: 0.9900 - val_loss: 0.0277 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 4049s 9s/step - loss: 0.0259 - accuracy: 0.9915 - val_loss: 0.0325 - val_accuracy: 0.9898\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0222 - accuracy: 0.9925 - val_loss: 0.0295 - val_accuracy: 0.9897\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.0326 - val_accuracy: 0.9892\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.0327 - val_accuracy: 0.9904\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0136 - accuracy: 0.9956 - val_loss: 0.0322 - val_accuracy: 0.9909\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0296 - val_accuracy: 0.9911\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.0296 - accuracy: 0.9911\n",
      "Test accuracy: 0.991100013256073\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize and reshape data\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "test_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tUse transfer learning for large image classification, going through these steps:\n",
    "- a.\tCreate a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "- b.\tSplit it into a training set, a validation set, and a test set.\n",
    "- c.\tBuild the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.\n",
    "- d.\tFine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743683d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
