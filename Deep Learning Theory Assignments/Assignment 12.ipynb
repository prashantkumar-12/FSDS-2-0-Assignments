{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 12 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tHow does unsqueeze help us to solve certain broadcasting problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7621eb",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Unsqueeze and Broadcasting:\n",
    "The unsqueeze operation adds a new dimension to a tensor, which can help solve broadcasting problems where dimensions don't match. It allows tensors with different shapes to be aligned properly for elementwise operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tHow can we use indexing to do the same operation as unsqueeze?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dda8785",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Indexing for Unsqueeze:\n",
    "You can achieve the same effect as unsqueeze using indexing. For example, tensor[:, None] would add a new dimension (axis) to the tensor along the second axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tHow do we show the actual contents of the memory used for a tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f1254",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "View Memory Contents of a Tensor:\n",
    "You can use the .storage() method or convert the tensor to a NumPy array using .numpy() to view the actual memory contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tWhen adding a vector of size 3 to a matrix of size 3Ã—3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff5543",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Adding Vector to Matrix:\n",
    "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each column of the matrix, replicating the vector along the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tDo broadcasting and expand_as result in increased memory use? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b350446",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Memory Usage with Broadcasting:\n",
    "Broadcasting and expand_as do not necessarily result in increased memory use. They enable efficient operations by avoiding explicit replication of data. They manipulate the way data is accessed, not necessarily duplicating it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tImplement matmul using Einstein summation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380efe83",
   "metadata": {},
   "source": [
    "**Answer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tWhat does a repeated index letter represent on the lefthand side of einsum?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4ac4b",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Repeated Index in einsum:\n",
    "A repeated index letter on the lefthand side of einsum indicates summation or contraction along that index. The index is summed over its range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tWhat are the three rules of Einstein summation notation? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101e982",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Three Rules of Einstein Summation Notation:\n",
    "\n",
    "Each index appears exactly twice (once as a subscript and once as a superscript).\n",
    "An index that is not repeated is called a \"free\" index.\n",
    "Repeated indices are summed over their range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e3af2",
   "metadata": {},
   "source": [
    "#### 9.\tWhat are the forward pass and backward pass of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b2db11",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Forward Pass and Backward Pass:\n",
    "The forward pass involves passing input through the neural network's layers to produce predictions. The backward pass (backpropagation) computes gradients of the loss with respect to the model's parameters, enabling optimization via gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bd31",
   "metadata": {},
   "source": [
    "#### 10.\tWhy do we need to store some of the activations calculated for intermediate layers in the forward pass?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052fa77c",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Storing Activations in Forward Pass:\n",
    "Storing intermediate activations is necessary for the backward pass during gradient computation. The gradients are calculated using these stored activations, enabling efficient computation of parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860f8b5",
   "metadata": {},
   "source": [
    "#### 11.\tWhat is the downside of having activations with a standard deviation too far away from 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13550435",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Downside of Deviating Standard Deviation:\n",
    "Activations with a standard deviation too far from 1 can lead to vanishing or exploding gradients during backpropagation, hindering convergence and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713869",
   "metadata": {},
   "source": [
    "#### 12.\tHow can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d63ba9",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Weight Initialization's Role:\n",
    "Proper weight initialization can help prevent activations from deviating too much. Techniques like He initialization initialize weights in a way that ensures activations remain within a reasonable range, aiding gradient flow and convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
