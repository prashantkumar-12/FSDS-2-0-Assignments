{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 15 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tDeep Learning.\n",
    "- a.\tBuild a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "- b.\tUsing Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "- c.\tTune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "- d.\tNow try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "- e.\tIs the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1b734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "861/861 [==============================] - 5s 4ms/step - loss: 0.1148 - accuracy: 0.9640 - val_loss: 0.0474 - val_accuracy: 0.9859\n",
      "Epoch 2/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0511 - accuracy: 0.9842 - val_loss: 0.0463 - val_accuracy: 0.9869\n",
      "Epoch 3/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0421 - accuracy: 0.9868 - val_loss: 0.0367 - val_accuracy: 0.9902\n",
      "Epoch 4/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0306 - accuracy: 0.9900 - val_loss: 0.0431 - val_accuracy: 0.9892\n",
      "Epoch 5/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0265 - accuracy: 0.9915 - val_loss: 0.0415 - val_accuracy: 0.9915\n",
      "Epoch 6/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.0355 - val_accuracy: 0.9918\n",
      "Epoch 7/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0217 - accuracy: 0.9931 - val_loss: 0.0396 - val_accuracy: 0.9912\n",
      "Epoch 8/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0181 - accuracy: 0.9943 - val_loss: 0.0413 - val_accuracy: 0.9902\n",
      "Epoch 9/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0153 - accuracy: 0.9955 - val_loss: 0.0313 - val_accuracy: 0.9922\n",
      "Epoch 10/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.0503 - val_accuracy: 0.9905\n",
      "Epoch 11/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0171 - accuracy: 0.9951 - val_loss: 0.0433 - val_accuracy: 0.9902\n",
      "Epoch 12/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.0513 - val_accuracy: 0.9912\n",
      "Epoch 13/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0278 - val_accuracy: 0.9928\n",
      "Epoch 14/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0162 - accuracy: 0.9956 - val_loss: 0.0464 - val_accuracy: 0.9899\n",
      "Epoch 15/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0268 - val_accuracy: 0.9941\n",
      "Epoch 16/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.0343 - val_accuracy: 0.9935\n",
      "Epoch 17/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0366 - val_accuracy: 0.9908\n",
      "Epoch 18/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0666 - val_accuracy: 0.9889\n",
      "Epoch 19/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0319 - val_accuracy: 0.9922\n",
      "Epoch 20/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0104 - accuracy: 0.9974 - val_loss: 0.0338 - val_accuracy: 0.9935\n",
      "Epoch 21/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.0481 - val_accuracy: 0.9925\n",
      "Epoch 22/100\n",
      "861/861 [==============================] - 3s 3ms/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 0.0390 - val_accuracy: 0.9925\n",
      "Epoch 23/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.0369 - val_accuracy: 0.9925\n",
      "Epoch 24/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0600 - val_accuracy: 0.9902\n",
      "Epoch 25/100\n",
      "861/861 [==============================] - 3s 4ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9938\n",
      "313/313 [==============================] - 1s 2ms/step - loss: nan - accuracy: 0.5100\n",
      "Test accuracy: 0.5099999904632568\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Filter data for digits 0 to 4\n",
    "filter_indices = (y_train_full <= 4)\n",
    "X_train, y_train = X_train_full[filter_indices], y_train_full[filter_indices]\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Build the DNN\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.1,\n",
    "                    callbacks=[early_stopping_cb, model_checkpoint_cb])\n",
    "\n",
    "# Load the best model\n",
    "model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# Evaluate the model on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Test accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tTransfer learning.\n",
    "- a.\tCreate a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
    "- b.\tTrain this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "- c.\tTry caching the frozen layers, and train the model again: how much faster is it now?\n",
    "- d.\tTry again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "- e.\tNow unfreeze the top two hidden layers and continue training: can you get the model to perform even better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a5b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "827/827 [==============================] - 3s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "827/827 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "827/827 [==============================] - 2s 3ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "827/827 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "827/827 [==============================] - 5s 6ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "827/827 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "827/827 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "313/313 [==============================] - 1s 4ms/step - loss: nan - accuracy: 0.0980\n",
      "Transfer model test accuracy: 0.09799999743700027\n"
     ]
    }
   ],
   "source": [
    "# Load the previously trained model (Part 1)\n",
    "pretrained_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# Freeze the pretrained hidden layers\n",
    "for layer in pretrained_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new output layer for the transfer learning task\n",
    "transfer_output = keras.layers.Dense(5, activation=\"softmax\")(pretrained_model.layers[-2].output)\n",
    "\n",
    "# Create the transfer learning model\n",
    "transfer_model = keras.Model(inputs=pretrained_model.input, outputs=transfer_output)\n",
    "\n",
    "# Compile the transfer model\n",
    "transfer_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=keras.optimizers.Adam(),\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "# Load digits 5 to 9 from MNIST\n",
    "(X_train_transfer, y_train_transfer), (X_test_transfer, y_test_transfer) = mnist.load_data()\n",
    "filter_indices_transfer = (y_train_transfer >= 5)\n",
    "X_train_transfer, y_train_transfer = X_train_transfer[filter_indices_transfer], y_train_transfer[filter_indices_transfer]\n",
    "X_train_transfer, X_test_transfer = X_train_transfer / 255.0, X_test_transfer / 255.0\n",
    "\n",
    "# Train the transfer model\n",
    "transfer_history = transfer_model.fit(X_train_transfer, y_train_transfer, epochs=100,\n",
    "                                      validation_split=0.1)\n",
    "\n",
    "# Evaluate the transfer model on test set\n",
    "test_loss_transfer, test_accuracy_transfer = transfer_model.evaluate(X_test_transfer, y_test_transfer)\n",
    "\n",
    "# Print test accuracy\n",
    "print(\"Transfer model test accuracy:\", test_accuracy_transfer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tPretraining on an auxiliary task.\n",
    "- a.\tIn this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. To do this, you should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function.\n",
    "- b.\tSplit the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
    "- c.\tTrain the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
    "- d.\tNow create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc317b59",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Train the auxiliary model on the auxiliary task\u001b[39;00m\n\u001b[0;32m     60\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m---> 61\u001b[0m auxiliary_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mgenerate_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_split_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Create a new model reusing and freezing DNN A's hidden layers\u001b[39;00m\n\u001b[0;32m     64\u001b[0m dnn_a_clone \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mclone_model(dnn_a)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mgenerate_batch\u001b[1;34m(X, batch_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m         y_batch[i] \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mequal(X_batch[i, \u001b[38;5;241m0\u001b[39m], X_batch[i, \u001b[38;5;241m1\u001b[39m]), tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         y_batch[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [X_batch[:, \u001b[38;5;241m0\u001b[39m], X_batch[:, \u001b[38;5;241m1\u001b[39m]], y_batch\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "X_train_full, X_test = X_train_full / 255.0, X_test / 255.0\n",
    "\n",
    "# Define the architecture for DNN A and DNN B\n",
    "dnn_a = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "])\n",
    "\n",
    "dnn_b = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "])\n",
    "\n",
    "# Create the concatenated hidden layer for auxiliary task\n",
    "concatenated = keras.layers.concatenate([dnn_a.output, dnn_b.output])\n",
    "hidden_layer_aux = keras.layers.Dense(10, activation=\"elu\", kernel_initializer=\"he_normal\")(concatenated)\n",
    "\n",
    "# Create the auxiliary task model\n",
    "auxiliary_model = keras.Model(inputs=[dnn_a.input, dnn_b.input], outputs=hidden_layer_aux)\n",
    "\n",
    "# Compile the auxiliary model\n",
    "auxiliary_model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# Split MNIST training set into two sets\n",
    "split_1_indices = (y_train_full < 5)\n",
    "X_train_split_1, y_train_split_1 = X_train_full[split_1_indices], y_train_full[split_1_indices]\n",
    "\n",
    "# Function to generate training batch for auxiliary task\n",
    "def generate_batch(X, batch_size):\n",
    "    batch_indices = tf.random.uniform(shape=[batch_size], maxval=len(X), dtype=tf.int32)\n",
    "    X_batch = X[batch_indices]\n",
    "    y_batch = tf.random.uniform(shape=[batch_size], maxval=2, dtype=tf.float32)\n",
    "    is_same_digit = tf.equal(y_batch, 0.0)\n",
    "    for i in range(batch_size):\n",
    "        if is_same_digit[i]:\n",
    "            y_batch[i] = tf.cast(tf.equal(X_batch[i, 0], X_batch[i, 1]), tf.float32)\n",
    "        else:\n",
    "            y_batch[i] = 1.0\n",
    "    return [X_batch[:, 0], X_batch[:, 1]], y_batch\n",
    "\n",
    "# Train the auxiliary model on the auxiliary task\n",
    "batch_size = 32\n",
    "auxiliary_model.fit(generate_batch(X_train_split_1, batch_size), epochs=10)\n",
    "\n",
    "# Create a new model reusing and freezing DNN A's hidden layers\n",
    "dnn_a_clone = keras.models.clone_model(dnn_a)\n",
    "dnn_a_clone.layers.pop()  # Remove the last layer\n",
    "transfer_output = dnn_a_clone.layers[-1].output\n",
    "transfer_model = keras.Model(inputs=dnn_a_clone.input, outputs=transfer_output)\n",
    "\n",
    "# Freeze the hidden layers of DNN A clone\n",
    "for layer in transfer_model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add a softmax output layer for classification\n",
    "output_layer_transfer = keras.layers.Dense(10, activation=\"softmax\")(transfer_output)\n",
    "transfer_model = keras.Model(inputs=transfer_model.input, outputs=output_layer_transfer)\n",
    "\n",
    "# Compile the transfer model\n",
    "transfer_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=keras.optimizers.Adam(),\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "# Split MNIST training set #2\n",
    "split_2_indices = (y_train_full >= 5)\n",
    "X_train_split_2, y_train_split_2 = X_train_full[split_2_indices], y_train_full[split_2_indices]\n",
    "\n",
    "# Train the transfer model on split #2\n",
    "transfer_model.fit(X_train_split_2, y_train_split_2, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724b179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
