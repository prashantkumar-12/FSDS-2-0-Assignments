{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 14 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f0a0d",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Weight Initialization with He Initialization:\n",
    "While He initialization randomizes weights, initializing all weights to the same value can still lead to symmetry breaking issues, especially in deep networks. It's better to randomize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it okay to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69675ecc",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Initializing Bias Terms to 0:\n",
    "It's generally okay to initialize bias terms to 0. However, in some cases, non-zero initialization might help with convergence or better learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the ELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c93638",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Advantages of ELU over ReLU:\n",
    "\n",
    "ELU has a non-zero gradient for negative inputs, preventing dying ReLU problem.\n",
    "ELU can take negative values, allowing it to capture more complex patterns.\n",
    "ELU has smoother gradients around zero, which can lead to faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69f8f5",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Activation Function Selection:\n",
    "\n",
    "ELU: When you want faster convergence and handle negative values.\n",
    "Leaky ReLU: Similar to ELU but simpler; can help prevent dead neurons.\n",
    "ReLU: Commonly used; simple and computationally efficient, but can suffer from dying neurons.\n",
    "Tanh: Useful in recurrent networks; outputs in range [-1, 1].\n",
    "Logistic: Often used in output layers for binary classification.\n",
    "Softmax: For multiclass classification; converts scores to class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26d5e7",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "High Momentum in MomentumOptimizer:\n",
    "Setting momentum too close to 1 can result in the optimizer overshooting and oscillating around the optimal point, slowing down convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20cd2c6",
   "metadata": {},
   "source": [
    "**Answers** \n",
    "Producing Sparse Model:\n",
    "\n",
    "L1 regularization: Encourages many weights to be exactly 0.\n",
    "Weight pruning: Removing small weights or connections.\n",
    "Sparse activation functions (e.g., ReLUs): Result in inactive neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6f9015",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "Dropout Impact on Training and Inference:\n",
    "\n",
    "Dropout can slow down training as fewer neurons contribute during each update.\n",
    "During inference, dropout is usually turned off, so it doesn't slow down predictions on new instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
