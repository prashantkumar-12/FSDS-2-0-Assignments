{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bd03c",
   "metadata": {},
   "source": [
    "# Assignment 03 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a248256",
   "metadata": {},
   "source": [
    "#### 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c0a77",
   "metadata": {},
   "source": [
    "**Answer**No, it is not recommended to initialize all the weights to the same value, even if they are selected randomly using He initialization. Initializing all weights to the same value would lead to symmetric neurons, and during the training process, all neurons in the same layer would end up learning the same features, limiting the model's capacity to learn and reducing its effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5f184",
   "metadata": {},
   "source": [
    "#### 2.\tIs it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90761eca",
   "metadata": {},
   "source": [
    "**Answer**No, it is generally not recommended to initialize bias terms to 0. Initializing biases to 0 can cause the neurons to output the same value for all inputs during the initial stages of training, leading to symmetry issues and hindering the learning process. It is preferable to initialize bias terms randomly to break symmetry and allow the neurons to learn different features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7aaa4",
   "metadata": {},
   "source": [
    "#### 3.\tName three advantages of the SELU activation function over ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68268a8e",
   "metadata": {},
   "source": [
    "**Answer**Advantages of the SELU (Scaled Exponential Linear Unit) activation function over ReLU:\n",
    "\n",
    "Vanishing Gradient: SELU activation helps mitigate the vanishing gradient problem, which can be an issue in deep networks using ReLU. SELU has a non-zero derivative for negative inputs, leading to better gradient flow during backpropagation.\n",
    "\n",
    "Stability: SELU has a self-normalizing property, ensuring that the output of each layer remains close to mean 0 and standard deviation 1 during training. This can lead to more stable learning and faster convergence.\n",
    "\n",
    "Zero-Centered Output: Unlike ReLU, which is always non-negative, SELU has a mean output close to zero. This zero-centered output can help prevent issues related to dead neurons and improve the overall performance of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cb7cc",
   "metadata": {},
   "source": [
    "#### 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191e1127",
   "metadata": {},
   "source": [
    "**Answer**Use Cases for Different Activation Functions:\n",
    "SELU (Scaled Exponential Linear Unit):\n",
    "Use in deep neural networks with many layers to mitigate vanishing gradient and ensure stable training. It is particularly effective in feedforward networks with dense connections.\n",
    "\n",
    "Leaky ReLU and its Variants (e.g., Parametric ReLU, Randomized Leaky ReLU):\n",
    "Use when training deep networks to prevent dead neurons and allow for better gradient flow. They are an improvement over ReLU, especially when dealing with negative inputs.\n",
    "\n",
    "ReLU (Rectified Linear Unit):\n",
    "Use as a default activation function for hidden layers in deep neural networks. ReLU helps in mitigating the vanishing gradient problem and has fast computation due to its piecewise linearity.\n",
    "\n",
    "tanh (Hyperbolic Tangent):\n",
    "Use when dealing with data that is centered around zero or has symmetric distributions. tanh maps inputs to a range between -1 and 1, making it suitable for tasks where negative values are meaningful.\n",
    "\n",
    "Logistic (Sigmoid):\n",
    "Use in binary classification problems where the output needs to be within a range of 0 to 1. It is commonly used in the output layer of binary classifiers.\n",
    "\n",
    "Softmax:\n",
    "Use in multi-class classification tasks to obtain class probabilities. Softmax activation ensures that the output probabilities sum to 1, making it suitable for multi-class problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cbd41",
   "metadata": {},
   "source": [
    "#### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f8d73",
   "metadata": {},
   "source": [
    "**Answer**Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer can lead to unstable and erratic behavior during training. The momentum hyperparameter controls the influence of past gradients on the current update step. When set too close to 1, the optimizer heavily relies on past gradients, and the updates can oscillate or diverge, preventing the optimizer from converging to the global minimum of the loss function. Such high momentum values can lead to overshooting the optimal solution and destabilize the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e479268",
   "metadata": {},
   "source": [
    "#### 6.\tName three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0fd233",
   "metadata": {},
   "source": [
    "**Answer**Three ways to produce a sparse model:\n",
    "\n",
    "L1 Regularization: Applying L1 regularization to the model's weights during training encourages many of the weights to become exactly zero. This results in a sparse model with only a subset of features being relevant for the task.\n",
    "\n",
    "Dropout: Dropout randomly deactivates neurons during training, forcing the model to learn robust representations by avoiding overreliance on any specific neurons. This can lead to some neurons being effectively \"turned off\" during training, creating a sparse representation.\n",
    "\n",
    "Pruning: Pruning involves removing or zeroing out less important weights or connections from the trained model. Pruning can be performed post-training based on the learned weights or during training using specialized techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71bd58a",
   "metadata": {},
   "source": [
    "#### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8446d9",
   "metadata": {},
   "source": [
    "**Answer**Dropout does not slow down training significantly. In fact, dropout can act as a form of regularization, preventing overfitting and improving generalization, which can lead to faster convergence in some cases.\n",
    "During inference (making predictions on new instances), dropout is typically turned off, and the full network is used. While dropout does not affect inference time, it may lead to different outputs for the same input due to the randomness introduced during training.\n",
    "\n",
    "MC Dropout (Monte Carlo Dropout) is a variant of dropout that involves running inference multiple times with dropout active. It can be used to estimate model uncertainty and generate probabilistic predictions. MC Dropout can increase inference time because multiple forward passes are needed to obtain uncertainty estimates. However, the inference time increase is generally manageable and can provide valuable uncertainty information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16db75",
   "metadata": {},
   "source": [
    "#### 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "1.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "2.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "3.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "4.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "5.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb637bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HeNormal\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m100\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43minput_shape\u001b[49m,), kernel_initializer\u001b[38;5;241m=\u001b[39mHeNormal()))\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Activation(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m19\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cfc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
