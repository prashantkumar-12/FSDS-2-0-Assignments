{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfde060b",
   "metadata": {},
   "source": [
    "# Assignment 12 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875c74b0",
   "metadata": {},
   "source": [
    "##### 1. What is prior probability? Give an example.\n",
    "**Answer**\n",
    "Prior probability is the initial belief or probability assigned to an event before considering new evidence. For example, the chance of flipping a fair coin and getting heads is 0.5 (50%) before any flips are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3589c8dd",
   "metadata": {},
   "source": [
    "##### 2. What is posterior probability? Give an example.\n",
    "**Answer**\n",
    "Posterior probability is the updated belief or probability of an event after taking into account new evidence or data. It is obtained by combining the prior probability with the likelihood of the evidence using Bayes' theorem.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say we have a bag of red and blue marbles, and we want to determine the probability of drawing a red marble from the bag. Initially, we believe that the bag contains an equal number of red and blue marbles, so the prior probability of drawing a red marble is 0.5 (50%).\n",
    "\n",
    "Now, we draw a marble from the bag without looking and observe that it is red. This new evidence updates our belief. Using Bayes' theorem, we combine the prior probability with the likelihood of drawing a red marble (assuming the probability of drawing red, given the bag's content) to get the updated probability or posterior probability of drawing a red marble. The posterior probability will no longer be 0.5 (50%) but will depend on the likelihood of drawing red marbles from the bag based on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7708f",
   "metadata": {},
   "source": [
    "##### 3. What is likelihood probability? Give an example.\n",
    "**Answer**\n",
    "Likelihood probability refers to the probability of observing a specific set of evidence or data given a particular hypothesis or model.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a six-sided fair die. The likelihood probability of rolling a 4, assuming the die is fair, is 1/6 (approximately 0.1667 or 16.67%). This means that if the die is fair, the probability of obtaining a 4 when rolling it is 1 out of 6 possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec66cef",
   "metadata": {},
   "source": [
    "##### 4. What is Naïve Bayes classifier? Why is it named so?\n",
    "**Answer**\n",
    "The Naïve Bayes classifier is a probabilistic machine learning algorithm used for classification tasks. It is based on Bayes' theorem and assumes that the features (attributes) are conditionally independent of each other given the class label.\n",
    "\n",
    "It is named \"Naïve\" because it makes the simplifying assumption of feature independence, which is often not true in real-world scenarios. Despite this oversimplified assumption, Naïve Bayes can perform surprisingly well and is computationally efficient, making it popular for text classification, spam filtering, and other simple classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b1a8f4",
   "metadata": {},
   "source": [
    "##### 5. What is optimal Bayes classifier?\n",
    "**Answer**\n",
    "The Optimal Bayes classifier, also known as the Bayes optimal classifier or Bayes optimal decision rule, is a theoretical concept in machine learning and statistics. It is considered the ideal classifier because it achieves the lowest possible error rate when given access to all available information.\n",
    "\n",
    "The Optimal Bayes classifier makes decisions based on the class with the highest posterior probability given the observed data and all possible class labels. In other words, it assigns a new data point to the class that has the highest probability of generating that data point, considering all available information.\n",
    "\n",
    "However, in practice, the Optimal Bayes classifier is often challenging to implement because it requires knowing the true underlying probability distributions, which are usually not available in real-world applications. As a result, approximations and simplifications like the Naïve Bayes classifier are often used in practical scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61196d",
   "metadata": {},
   "source": [
    "##### 6. Write any two features of Bayesian learning methods.\n",
    "**Answer**\n",
    "Incorporation of prior knowledge: Bayesian learning methods allow the incorporation of prior knowledge or beliefs about the problem domain, which can be helpful in making more informed and accurate predictions.\n",
    "\n",
    "Probabilistic framework: Bayesian learning methods use a probabilistic framework to model uncertainty, allowing them to provide not only predictions but also estimates of the uncertainty associated with those predictions. This is particularly useful in decision-making and risk assessment tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab745e",
   "metadata": {},
   "source": [
    "##### 7. Define the concept of consistent learners.\n",
    "**Answer**\n",
    "Consistent learners, also known as asymptotically consistent learners, are machine learning algorithms that, as the size of the training data increases, converge to the true underlying model or function that generated the data. In other words, as the amount of data available for training grows, consistent learners approach the best possible predictive performance and make fewer errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250e462",
   "metadata": {},
   "source": [
    "##### 8. Write any two strengths of Bayes classifier.\n",
    "**Answer**\n",
    "Simplicity and efficiency: Bayes classifiers, especially the Naïve Bayes variant, are simple and computationally efficient. They require a relatively small amount of training data and can handle high-dimensional feature spaces effectively, making them suitable for quick prototyping and real-time applications.\n",
    "\n",
    "Probabilistic interpretation: Bayes classifiers provide a probabilistic interpretation of their predictions. They offer not only the predicted class label but also the associated probabilities, which can be valuable in decision-making and risk assessment tasks, allowing users to make more informed choices based on uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0b469",
   "metadata": {},
   "source": [
    "##### 9. Write any two weaknesses of Bayes classifier.\n",
    "**Answer**\n",
    "Naïve independence assumption: The Naïve Bayes classifier assumes that all features are conditionally independent given the class label. This assumption may not hold true in many real-world scenarios, leading to suboptimal performance when features are actually correlated.\n",
    "\n",
    "Sensitivity to irrelevant features: Bayes classifiers can be sensitive to irrelevant features in the data. Since they use all features to make predictions, even those that do not contribute useful information, the inclusion of irrelevant features may adversely affect the classifier's accuracy. Feature selection or engineering techniques are often needed to mitigate this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9bb179",
   "metadata": {},
   "source": [
    "##### 10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "        1. Text classification\n",
    "\n",
    "        2. Spam filtering\n",
    "\n",
    "       3. Market sentiment analysis\n",
    "**Answer**\n",
    "Text Classification:\n",
    "In text classification, the Naïve Bayes classifier is used to categorize text documents into predefined classes or categories. The algorithm works by considering the frequency of each word (feature) in the document and the class label. It uses the Bayes theorem to calculate the posterior probability of a document belonging to a specific class given its word frequencies. The class with the highest posterior probability is then assigned to the document.\n",
    "For example, consider a sentiment analysis task where we want to classify movie reviews as either positive or negative. The Naïve Bayes classifier would analyze the frequency of words in positive and negative movie reviews during the training phase. Then, when given a new review, it would calculate the probabilities of the words in the review belonging to each class and determine whether it is more likely to be positive or negative.\n",
    "\n",
    "Spam Filtering:\n",
    "In spam filtering, the Naïve Bayes classifier is employed to determine whether an incoming email is spam or not. The classifier uses the occurrence of certain words or features in the email (such as specific keywords or phrases) to assess the probability of the email being spam. During the training phase, the classifier learns from a labeled dataset containing examples of both spam and non-spam emails.\n",
    "For instance, if the word \"discount\" or \"offer\" appears frequently in spam emails but rarely in legitimate ones, the Naïve Bayes classifier will assign a higher probability of the email being spam if these words are present. By combining the probabilities of multiple features, the classifier makes a final decision about whether the email should be classified as spam or not.\n",
    "\n",
    "Market Sentiment Analysis:\n",
    "Market sentiment analysis involves gauging the overall sentiment (positive, negative, or neutral) of market participants about a particular financial asset or market. Sentiment analysis can be performed on social media posts, news articles, or financial reports to understand the market sentiment towards a specific stock, currency, or commodity.\n",
    "In this context, the Naïve Bayes classifier is used to classify individual pieces of text (e.g., tweets, news headlines) as expressing positive, negative, or neutral sentiments towards the asset. The classifier learns from a labeled dataset of sentiment-labeled text and then calculates the likelihood of the observed words or phrases in a new text expressing a particular sentiment. The sentiment with the highest posterior probability is assigned to the text, indicating the overall market sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d7d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6f125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7317a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65192e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef5b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a041749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3007e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
