{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed39bab6",
   "metadata": {},
   "source": [
    "# Assignment 5 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c3bf0",
   "metadata": {},
   "source": [
    "##### 1. What are the key tasks that machine learning entails? What does data pre-processing imply ?\n",
    "**Answers**\n",
    "There are Five core tasks in the common ML workflow:\n",
    "\n",
    "- Get Data: The first step in the Machine Learning process is getting data.\n",
    "- Cleaning, Preparing & Manipulating Data: Real-world data often has unorganized, missing, or noisy elements.\n",
    "- Train Model: This step is where the magic happens!\n",
    "- Testing Model.\n",
    "- Improving model.\n",
    "- Data preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "- Preprocessing involves both data validation and data imputation\n",
    "- The Goal of Data Validation is to assess whether the data in question is both complete and accurate.\n",
    "- The Goal of Data Imputation is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379170f",
   "metadata": {},
   "source": [
    "##### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two.\n",
    "**Answers**\n",
    "Quantitative data is a type of data that deals with numerical values representing measurable quantities or attributes. It is based on quantities and can be expressed in terms of numbers. This data is used for mathematical calculations, statistical analysis, and modeling. Quantitative data can be further categorized into two subtypes:\n",
    "\n",
    "Continuous Data:\n",
    "\n",
    "Continuous data is data that can take any value within a specific range, and it can be measured with an infinite number of decimal places.\n",
    "There are no gaps or interruptions between data points in continuous data.\n",
    "Examples of continuous data include measurements like temperature (e.g., 25.5Â°C), weight (e.g., 65.2 kg), time (e.g., 3.75 seconds), and height (e.g., 165.8 cm).\n",
    "Continuous data is typically represented on a continuous scale, such as a number line.\n",
    "Discrete Data:\n",
    "\n",
    "Discrete data is data that can only take specific, distinct values, and it is often represented in whole numbers.\n",
    "There are gaps or interruptions between data points in discrete data.\n",
    "Examples of discrete data include counts (e.g., number of students in a class), scores on a test (e.g., 85, 90, 95), number of products sold, and number of cars in a parking lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e7f1ab",
   "metadata": {},
   "source": [
    "##### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types.\n",
    "**Answers**\n",
    "Data Collection: Student Information\n",
    "\n",
    "Student ID\tGender\tAge\tGrade\tHeight (cm)\tNumber of Siblings\n",
    "001\tMale\t17\t12\t175.2\t2\n",
    "002\tFemale\t16\t11\t160.5\t1\n",
    "003\tMale\t18\t12\t182.0\t0\n",
    "004\tFemale\t17\t12\t163.7\t3\n",
    "005\tMale\t16\t11\t170.9\t2\n",
    "006\tFemale\t18\t12\t168.3\t1\n",
    "007\tMale\t17\t12\t176.8\t2\n",
    "008\tFemale\t16\t11\t158.9\t1\n",
    "Explanation of Attributes:\n",
    "\n",
    "Student ID: This attribute is a unique identifier for each student. It is a discrete quantitative attribute representing unique numeric values.\n",
    "\n",
    "Gender: This attribute represents the gender of the student. It is a qualitative nominal attribute, as there is no inherent order or ranking between male and female categories.\n",
    "\n",
    "Age: This attribute represents the age of the student. It is a discrete quantitative attribute since age is measured in whole numbers.\n",
    "\n",
    "Grade: This attribute represents the grade level of the student. It is a qualitative ordinal attribute, as grade levels have a specific order (e.g., 11th grade comes before 12th grade).\n",
    "\n",
    "Height (cm): This attribute represents the height of the student in centimeters. It is a continuous quantitative attribute since height can take any value within a specific range and can be measured with decimals.\n",
    "\n",
    "Number of Siblings: This attribute represents the number of siblings the student has. It is a discrete quantitative attribute since the number of siblings is measured in whole numbers.\n",
    "\n",
    "This data collection provides a simple example of various machine learning data types commonly encountered in real-world datasets. The attributes cover both qualitative and quantitative aspects, making it suitable for a variety of analysis and modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa987d",
   "metadata": {},
   "source": [
    "##### 4. What are the various causes of machine learning data issues? What are the ramifications?\n",
    "**Answers**\n",
    "Various causes of machine learning data issues include:\n",
    "\n",
    "Missing Data: Data may have missing values due to various reasons like human errors during data entry, data corruption, or certain values being unavailable or not recorded.\n",
    "\n",
    "Duplicate Data: Duplicated records or entries can arise when merging datasets from different sources or due to data recording errors.\n",
    "\n",
    "Outliers: Outliers are extreme values that deviate significantly from the rest of the data. They can occur due to errors, data corruption, or genuine rare occurrences.\n",
    "\n",
    "Imbalanced Data: Imbalanced datasets have a disproportionate distribution of classes, where one class is significantly more prevalent than others, leading to biased model training.\n",
    "\n",
    "Inconsistent Data: Data collected from different sources or at different times may have inconsistencies in naming conventions, units of measurement, or data formats.\n",
    "\n",
    "Incorrect Labels: In supervised learning, incorrect labels assigned to data instances can mislead the model during training and lead to reduced accuracy.\n",
    "\n",
    "Feature Engineering Errors: Errors in creating or selecting features may result in irrelevant or noisy data being used for training the model.\n",
    "\n",
    "Data Leakage: Information from the target variable unintentionally seeping into the features can lead to overly optimistic model performance.\n",
    "\n",
    "Limited Data: Insufficient data can result in models that do not generalize well to new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a850dd",
   "metadata": {},
   "source": [
    "##### 5. Demonstrate various approaches to categorical data exploration with appropriate examples.\n",
    "**Answers**\n",
    "1. Frequency Distribution:\n",
    "Calculate the frequency of each category to see how often it appears in the dataset. This approach helps understand the relative occurrence of different categories.\n",
    "\n",
    "Example: Let's consider a dataset of survey responses where participants were asked to choose their favorite color from a list. We can create a frequency distribution table to see how many participants chose each color:\n",
    "\n",
    "Color\tFrequency\n",
    "Red\t15\n",
    "Blue\t20\n",
    "Green\t10\n",
    "Yellow\t8\n",
    "Orange\t5\n",
    "2. Bar Chart:\n",
    "Visualize the frequency distribution using a bar chart, which makes it easier to compare the categories visually.\n",
    "\n",
    "Example: Using the same survey data, we can plot a bar chart to visualize the frequency of each color:\n",
    "\n",
    "\n",
    "\n",
    "3. Pie Chart:\n",
    "A pie chart displays the proportion of each category as a slice of the entire pie, making it useful for visualizing the relative distribution of categories.\n",
    "\n",
    "Example: Continuing with the survey data, we can plot a pie chart to show the percentage of participants who chose each color:\n",
    "\n",
    "\n",
    "4. Stacked Bar Chart:\n",
    "If we have multiple categorical variables and want to see their distribution collectively, we can use a stacked bar chart.\n",
    "\n",
    "Example: Suppose we have survey data on favorite colors and favorite fruits. We can create a stacked bar chart to see how the preferences are distributed across the different categories:\n",
    "\n",
    "\n",
    "\n",
    "5. Cross Tabulation:\n",
    "Cross tabulation, or a contingency table, shows the relationship between two categorical variables and the count of occurrences for each combination.\n",
    "\n",
    "Example: Let's consider a dataset with two categorical variables: \"Gender\" and \"Color Preference.\" We can create a cross-tabulation table to see how many participants from each gender prefer each color:\n",
    "\n",
    "Red\tBlue\tGreen\tYellow\tOrange\n",
    "Male\t10\t12\t8\t6\t3\n",
    "Female\t5\t8\t2\t2\t2\n",
    "6. Grouped Bar Chart:\n",
    "A grouped bar chart visually compares the frequency of different categories for each group in the dataset.\n",
    "\n",
    "Example: Using the same \"Gender\" and \"Color Preference\" data, we can create a grouped bar chart to compare color preferences between males and females:\n",
    "\n",
    "\n",
    "\n",
    "7. Heatmap:\n",
    "A heatmap can be used to visualize the strength of relationships between two categorical variables, providing a color-coded representation of the frequency counts.\n",
    "\n",
    "Example: Let's consider a dataset with two categorical variables: \"Country\" and \"Product Category.\" We can create a heatmap to show the frequency of each product category sold in each country:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655770ce",
   "metadata": {},
   "source": [
    "##### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?\n",
    "**Answers**\n",
    "To address the issue of missing values, several strategies can be employed:\n",
    "\n",
    "Complete Case Analysis: One simple approach is to remove data instances with missing values (complete case analysis). While this may be suitable for datasets with a small proportion of missing values, it can result in a significant loss of data and may not be ideal for datasets with a substantial amount of missing information.\n",
    "\n",
    "Imputation Techniques: Imputation involves filling in the missing values with estimated or predicted values. Various imputation techniques exist, such as mean imputation, median imputation, mode imputation, and regression imputation. These methods help retain more data while addressing the issue of missingness. However, the choice of imputation technique should be carefully considered as it can introduce bias if not applied appropriately.\n",
    "\n",
    "Multiple Imputation: Multiple imputation is a more sophisticated approach that involves generating multiple imputed datasets and combining the results to provide more accurate estimates and account for uncertainty introduced by the imputation process.\n",
    "\n",
    "Indicator Variable: For categorical data, an additional indicator variable can be created to represent the presence or absence of missing values. This way, the model can capture the information about missingness as a separate category.\n",
    "\n",
    "Missingness as a Feature: Depending on the dataset and the nature of the missing values, missingness can itself be considered as a feature. For example, the presence or pattern of missingness may hold valuable information and could be included as an input feature to the model.\n",
    "\n",
    "Model-Based Imputation: More advanced techniques involve using machine learning models to predict missing values based on other available features. For example, regression models, decision trees, or k-nearest neighbors can be used for imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4f91c",
   "metadata": {},
   "source": [
    "##### 7. Describe the various methods for dealing with missing data values in depth.\n",
    "**Answers**\n",
    "Dealing with missing data values is a crucial step in data preprocessing, as it can significantly impact the quality and accuracy of machine learning models. There are various methods for handling missing data, each with its advantages and limitations. Let's explore these methods in depth:\n",
    "\n",
    "1. Deletion Methods:\n",
    "\n",
    "Listwise Deletion (Complete Case Analysis): In this approach, data instances with missing values in any of the features are removed from the dataset. It can lead to a loss of valuable information, especially if missingness is not random. Listwise deletion is simple but may not be suitable when the proportion of missing values is large.\n",
    "Pairwise Deletion: In this approach, data instances with missing values are excluded only from analyses involving the specific feature with missing data. It retains more data than listwise deletion, but the sample size can vary across analyses.\n",
    "2. Mean/Median/Mode Imputation:\n",
    "\n",
    "Mean Imputation: Missing values in a numerical feature are replaced with the mean value of that feature across the dataset. It is a straightforward method but can introduce bias and underestimate the variance of the data.\n",
    "Median Imputation: Similar to mean imputation, but missing values are replaced with the median value of the feature. It is less sensitive to outliers than mean imputation.\n",
    "Mode Imputation: Missing values in categorical features are replaced with the mode (most frequent category) of that feature. It is appropriate for nominal data.\n",
    "3. Regression Imputation:\n",
    "\n",
    "A regression model is used to predict missing values in a feature based on other features. It is particularly useful when there is a correlation between the missing feature and other variables. Multiple regression and multiple imputation techniques can be employed for better accuracy.\n",
    "4. K-Nearest Neighbors (KNN) Imputation:\n",
    "\n",
    "KNN imputation predicts missing values based on the values of their nearest neighbors in the feature space. It works well for both numerical and categorical data and preserves the data structure.\n",
    "5. Hot Deck Imputation:\n",
    "\n",
    "Hot deck imputation assigns missing values based on values of similar instances in the same dataset. It can be random hot deck (assigning values from randomly selected similar instances) or deterministic hot deck (assigning values from the most similar instance).\n",
    "6. Multiple Imputation:\n",
    "\n",
    "Multiple imputation involves generating multiple imputed datasets using a chosen imputation method. The model is then trained on each dataset, and the results are combined to account for uncertainty in the imputation process. This approach provides more accurate estimates and incorporates variability due to missing data.\n",
    "7. Markov Chain Monte Carlo (MCMC) Imputation:\n",
    "\n",
    "MCMC imputation is a sophisticated method based on Bayesian modeling. It generates multiple imputations by iteratively sampling from the posterior distribution of the missing values, considering both observed and imputed data.\n",
    "8. Model-Based Imputation:\n",
    "\n",
    "In this approach, machine learning models (e.g., decision trees, random forests) are used to predict missing values based on other features. The model is trained on instances with complete data and used to predict missing values.\n",
    "It is essential to choose the appropriate method based on the type and pattern of missing data, the relationship between variables, and the nature of the machine learning task. No single method is universally superior, and the impact of imputation should be carefully evaluated to ensure it does not introduce bias or lead to erroneous conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8283b",
   "metadata": {},
   "source": [
    "##### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words.\n",
    "**Answers**\n",
    "Various data pre-processing techniques include:\n",
    "\n",
    "Data Cleaning: Removing or correcting errors, handling missing values, and dealing with outliers to improve data quality.\n",
    "\n",
    "Data Transformation: Scaling, normalization, or log transformation to bring the data to a common scale and improve model performance.\n",
    "\n",
    "Encoding Categorical Variables: Converting categorical data into numerical form for model compatibility.\n",
    "\n",
    "Feature Engineering: Creating new features from existing ones to improve the model's ability to capture patterns.\n",
    "\n",
    "Dimensionality Reduction: Reducing the number of features to simplify the model and avoid overfitting.\n",
    "\n",
    "Data Sampling: Balancing imbalanced datasets to prevent biased model training.\n",
    "\n",
    "Feature Selection: Selecting the most relevant features to improve model efficiency and interpretability.\n",
    "\n",
    "Data Integration: Combining data from multiple sources into a single cohesive dataset.\n",
    "\n",
    "Dimensionality Reduction:\n",
    "Dimensionality reduction is the process of reducing the number of features (variables) in a dataset while preserving most of the essential information. It aims to simplify the model, improve computational efficiency, and avoid the curse of dimensionality. Techniques like Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) are used for dimensionality reduction.\n",
    "\n",
    "Feature Selection:\n",
    "Feature selection is the process of choosing a subset of the most relevant features from the original feature set. It helps in improving model performance by reducing noise, overfitting, and computational complexity. Feature selection can be achieved through techniques like Recursive Feature Elimination (RFE), Lasso regression, and mutual information-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8dc96a",
   "metadata": {},
   "source": [
    "##### 9. i. What is the IQR? What criteria are used to assess it?\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker    surpass the upper whisker in length? How can box plots be used to identify outliers?\n",
    "**Answers**\n",
    "The IQR stands for Interquartile Range, which is a statistical measure used to assess the spread or dispersion of a dataset. It is based on the concept of quartiles, which are values that divide a dataset into four equal parts.\n",
    "\n",
    "The IQR is calculated as the difference between the third quartile (Q3) and the first quartile (Q1). In other words:\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "Here's how the IQR is assessed:\n",
    "\n",
    "Measuring Spread: The IQR provides information about the spread of the middle 50% of the data. It gives an indication of how much the data values deviate from the median.\n",
    "\n",
    "Identifying Outliers: The IQR is often used to detect outliers in a dataset. Data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers.\n",
    "\n",
    "Robustness: The IQR is a robust measure of spread because it is not influenced by extreme values or outliers. It is less sensitive to extreme observations compared to the range or standard deviation.\n",
    "\n",
    "Box Plot: The IQR is used to construct box plots, where the box represents the interquartile range, and the \"whiskers\" extend to the most extreme data points within a certain range.\n",
    "\n",
    "Comparing Data Sets: The IQR can be used to compare the spread of different datasets. A larger IQR indicates a more dispersed dataset, while a smaller IQR indicates a more concentrated dataset.\n",
    "\n",
    "Non-Normal Data: The IQR is valuable when dealing with non-normally distributed data, as it does not assume a specific data distribution.\n",
    "\n",
    "Statistical Analysis: The IQR is used in various statistical analyses and is often employed in conjunction with other measures of spread and central tendency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdafe8c",
   "metadata": {},
   "source": [
    "##### 10. Make brief notes on any two of the following:\n",
    "\n",
    "              1. Data collected at regular intervals\n",
    "\n",
    "               2. The gap between the quartiles\n",
    "\n",
    "               3. Use a cross-tab\n",
    "\n",
    "1. Make a comparison between:\n",
    "\n",
    "1. Data with nominal and ordinal values\n",
    "\n",
    "2. Histogram and box plot\n",
    "\n",
    "3. The average and median\n",
    "**Answers**\n",
    "1. Data collected at regular intervals:\n",
    "\n",
    "Data collected at regular intervals refers to data points recorded at consistent and fixed time intervals or spatial intervals.\n",
    "Examples of data collected at regular intervals include temperature measurements taken every hour, daily stock market prices, or monthly sales figures.\n",
    "Regular interval data is often used in time series analysis to identify patterns, trends, and seasonal variations over time.\n",
    "Time-based data can be effectively visualized using line charts or time series plots to observe trends and patterns over a continuous period.\n",
    "2. The gap between the quartiles:\n",
    "\n",
    "The gap between the quartiles refers to the difference between the first quartile (Q1) and the third quartile (Q3) in a dataset, i.e., IQR = Q3 - Q1.\n",
    "The interquartile range (IQR) is a measure of the spread or dispersion of the middle 50% of the data.\n",
    "It is used in box plots to show the range within which most of the data lies, with the box representing the IQR.\n",
    "The IQR is robust against outliers, making it a useful measure for assessing the spread of non-normally distributed data.\n",
    "\n",
    "Comparison between:\n",
    "\n",
    "1. Data with nominal and ordinal values:\n",
    "\n",
    "Nominal data consists of categories with no inherent order or ranking, e.g., colors, gender, or country names.\n",
    "Ordinal data consists of categories with a meaningful order or ranking, but the differences between categories may not be uniform, e.g., educational levels (elementary, high school, bachelor's degree).\n",
    "Nominal data can be represented using bar charts or pie charts, whereas ordinal data can also use these visualizations but is more suitable for ordered bar charts to show the ranking.\n",
    "2. Histogram and box plot:\n",
    "\n",
    "Histograms and box plots are both used to display the distribution of numerical data.\n",
    "A histogram is a graphical representation of the frequency distribution of continuous or discrete data. It consists of a series of bars, where the area of each bar represents the frequency of data within a specific bin or range of values.\n",
    "A box plot, also known as a box-and-whisker plot, provides a visual summary of the data's distribution through five summary statistics (minimum, Q1, median, Q3, maximum) and the interquartile range (IQR).\n",
    "Histograms are suitable for showing the shape of the distribution and detecting skewness, while box plots are useful for identifying outliers and visualizing the spread and central tendency of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0351f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8ddd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b79691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9080c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237572c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
