{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 20 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is the underlying concept of Support Vector Machines ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe015239",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "Underlying Concept of Support Vector Machines (SVM):\n",
    "SVM is a powerful supervised learning algorithm used for classification and regression tasks. The main concept of SVM is to find an optimal hyperplane that best separates data points of different classes in a high-dimensional space. It aims to maximize the margin (distance) between the hyperplane and the closest data points, known as support vectors, to improve the model's generalization and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is the concept of a support vector ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea66b18",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "Concept of a Support Vector:\n",
    "A support vector is a data point that lies closest to the decision boundary (hyperplane) in SVM. These points are critical for defining the optimal hyperplane because they directly influence the margin. Only support vectors significantly impact the decision boundary; other data points do not affect the model's parameters. In essence, support vectors \"support\" the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. When using SVMs, why is it necessary to scale the inputs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c373f9",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "Scaling Inputs in SVMs:\n",
    "Scaling the inputs (features) in SVMs is necessary to ensure that all features contribute equally to the model's training process. Since SVM aims to maximize the margin, features with larger scales may dominate the optimization process. By scaling the inputs to a common range (e.g., 0 to 1 or -1 to 1), SVM treats all features with equal importance, leading to a more balanced and accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80647a3b",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "Confidence Score and Percentage Chance in SVM Classifier:\n",
    "In SVM, the output of the classifier for a given data point is the predicted class label. SVM is primarily a binary classifier, meaning it assigns data points to one of two classes. While SVM can classify data points confidently, it does not inherently output a probability or percentage chance for a specific class. To obtain probability estimates, techniques like Platt scaling or probability calibration methods can be applied to the SVM's decision scores to approximate class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac7ac9",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "Primal or Dual Form for Large Datasets:\n",
    "For large datasets with millions of instances and hundreds of features, using the dual form of the SVM problem is generally preferred. The dual form allows the SVM to represent the data using the dot products between the support vectors, which is more efficient for high-dimensional datasets. The primal form involves solving for the weight vector directly in the feature space and can become computationally intensive for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde37ab0",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "\n",
    "Underfitting and RBF Kernel in SVM:\n",
    "To address underfitting with an RBF kernel, you should increase the value of gamma. Higher gamma values make the decision boundary more flexible, potentially capturing more complex patterns in the data.\n",
    "For the regularization parameter C, you should decrease its value. Lower C values increase the regularization strength, allowing the SVM to tolerate more misclassifications and find a broader margin, which can help combat underfitting.\n",
    "QP Parameters for Soft Margin Linear SVM:\n",
    "H: The Hessian matrix, computed from the dot products of the training samples (X) using the chosen kernel function (e.g., linear kernel for soft margin linear SVM).\n",
    "f: The vector representing the coefficients of the linear objective function for the QP solver.\n",
    "A: The matrix representing the equality constraint (should be a diagonal matrix with 1s, as there are no equality constraints).\n",
    "b: The vector representing the right-hand side of the equality constraint (typically set to 1).\n",
    "Training Different Linear Models on Linearly Separable Dataset:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b0ad7f",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "QP Parameters for Soft Margin Linear SVM:\n",
    "H: The Hessian matrix, computed from the dot products of the training samples (X) using the chosen kernel function (e.g., linear kernel for soft margin linear SVM).\n",
    "f: The vector representing the coefficients of the linear objective function for the QP solver.\n",
    "A: The matrix representing the equality constraint (should be a diagonal matrix with 1s, as there are no equality constraints).\n",
    "b: The vector representing the right-hand side of the equality constraint (typically set to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fbfba",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "Training Different Linear Models on Linearly Separable Dataset:\n",
    "LinearSVC: Trains a linear support vector classifier using the one-vs-rest (OvR) strategy for multi-class classification.\n",
    "SVC with linear kernel: Same as LinearSVC, but explicitly using a linear kernel.\n",
    "SGDClassifier: Trains a linear classifier using the stochastic gradient descent optimization method.\n",
    "When trained on the same linearly separable dataset, all three models should produce similar decision boundaries as they essentially learn linear classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93502c",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "SVM Classifier on MNIST Dataset:\n",
    "Due to SVM classifiers being binary classifiers, you'll need to use one-versus-the-rest (OvR) strategy to classify all 10 digits.\n",
    "To accelerate the process, use small validation sets for hyperparameter tuning.\n",
    "Achieving high precision depends on hyperparameter tuning, feature scaling, and model complexity. Precision values above 90% are possible with well-tuned SVM classifiers on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026cc3c",
   "metadata": {},
   "source": [
    "**Answer*\n",
    "\n",
    "SVM Regressor on California Housing Dataset:\n",
    "Train an SVM regressor on the California housing dataset to predict the median house value. The SVM regressor uses a linear or non-linear kernel based on the hyperparameters. The model aims to find the best-fit hyperplane or decision function to predict the target variable (house values)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
